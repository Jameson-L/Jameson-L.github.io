<!DOCTYPE html>
<html>
<head>
    <title>Project 4</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-chtml.js"></script>

    <h1>Project 4: Neural Radiance Field</h1>
    <h2>Jameson Liu</h2>

    <h3>Part 0: Calibrating Your Camera and Capturing a 3D Scan</h3>

    <p>
        To create a dataset for NeRF, I first took many images of ArUco tags to calibrate my camera and obtain an intrinsics matrix.
        Then, I took many images of an object of my choice with an ArUco tag and recovered the extrinsics matrices. 
    </p>
    <p>
        I found that the images had black borders after undistortion, so I cropped them out. 
        Additionally, since my images were very high resolution (5712Ã—4284), I used the full-resolution images for calibration and pose estimation, and then downsampled them afterward.
        After both of these steps, I adjusted the intrinsics accordingly. 
    </p>
    <p>
        Finally, the intrinsics, extrinsics, and images (undistorted) were saved together into a dataset for later use.
    </p>    
    
    <h4>My Object</h4>
    <div class="img-group" style="max-width: 1000px">
        <div class="img-item">
            <img src="assets/octopus_nerf/IMG_7335.jpg">
        </div>
    </div>

    <h4>Camera Poses/Frustums (Viser)</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/octopus_nerf/viser1.jpg">
        </div>
        <div class="img-item">
            <img src="assets/octopus_nerf/viser2.jpg">
        </div>
    </div>

    <h3>Part 1: Fit a Neural Field to a 2D Image</h3>

    <p>
        To practice using neural networks, I fitted a neural field to the provided fox image and my own image.
        To perform hyperparameter tuning, I experimented with different numbers of frequencies for positional encoding (L) and hidden layer width. 
    </p>

    <h4>Hyperparameter Tuning on Fox Image</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/fox_2_128.jpg">
            <p><em>L=2, Width=128</em></p>
        </div>
        <div class="img-item">
            <img src="assets/fox_2_256.jpg">
            <p><em>L=2, Width=256</em></p>
        </div>
    </div>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/fox_10_128.jpg">
            <p><em>L=10, Width=128</em></p>
        </div>
        <div class="img-item">
            <img src="assets/fox_10_256/iteration_2000.jpg">
            <p><em>L=10, Width=256</em></p>
        </div>
    </div>

    <h4>Chosen Model Architecture/Parameters</h4>
    <ul>
        <li>Number of (linear) layers = 4</li>
        <li>Width = 256</li>
        <li>Learning rate = 1e-2</li>
        <li>Batch size = 10000</li>
        <li>L = 10</li>
    </ul>

    <h4>Intermediate Renders of Fox Image</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/fox_10_256/iteration_0.jpg">
            <p><em>Iteration 0</em></p>
        </div>
        <div class="img-item">
            <img src="assets/fox_10_256/iteration_500.jpg">
            <p><em>Iteration 500</em></p>
        </div>
        <div class="img-item">
            <img src="assets/fox_10_256/iteration_1000.jpg">
            <p><em>Iteration 1000</em></p>
        </div>
        <div class="img-item">
            <img src="assets/fox_10_256/iteration_1500.jpg">
            <p><em>Iteration 1500</em></p>
        </div>
        <div class="img-item">
            <img src="assets/fox_10_256/iteration_2000.jpg">
            <p><em>Iteration 2000</em></p>
        </div>
    </div>

    <h4>Training Curves</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/fox_10_256/training_curves.jpg">
        </div>
    </div>

    <h4>Intermediate Renders of My Own Image</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/octopus_10_256/iteration_0.jpg">
            <p><em>Iteration 0</em></p>
        </div>
        <div class="img-item">
            <img src="assets/octopus_10_256/iteration_500.jpg">
            <p><em>Iteration 500</em></p>
        </div>
        <div class="img-item">
            <img src="assets/octopus_10_256/iteration_1000.jpg">
            <p><em>Iteration 1000</em></p>
        </div>
        <div class="img-item">
            <img src="assets/octopus_10_256/iteration_1500.jpg">
            <p><em>Iteration 1500</em></p>
        </div>
        <div class="img-item">
            <img src="assets/octopus_10_256/iteration_2000.jpg">
            <p><em>Iteration 2000</em></p>
        </div>
    </div>

    <h4>Training Curves</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/octopus_10_256/training_curves.jpg">
        </div>
    </div>

    <h3>Part 2: Fit a Neural Radiance Field from Multi-view Images</h3>

    <p>
        Creating a neural radiance field from a dataset of multi-view images involved several steps:        
    </p>
    <p>
        I first computed the rays corresponding to each pixel of an image.
        This involved first converting the pixels to camera coordinates using camera intrinsics. 
        Since pixels don't have depth, the depth was a free variable. 
        Following this, I converted the camera coordinates to world coordinates using the extrinsics matrix.
        Finally, to find the ray direction, I picked a point along the ray at a depth of 1. 
    </p>
    <p>
        Then, to sample data, I implemented random sampling of images and then rays from those images. 
        After getting a sample of rays, I sampled points along those rays with a configurable range of distances.
    </p>
    <p>
        With this functionality, I created a dataloader that could be queried to fetch data (points along rays and corresponding pixel ground truths) in batches. 
        These batches were given to the neural network during training.
    </p>
    <p>
        The actual neural radiance field was generated by a neural network which had 8 linear layers followed by 2 branches, for density and color estimation. 
        The inputs were 3D coordinates and a viewing direction, but both of these vectors were encoded using positional encoding so the model could capture high-frequency details. 
        
        $$PE(x) = \{x, sin(2^0\pi x), cos(2^0\pi x), sin(2^1\pi x), cos(2^1\pi x), ..., sin(2^{L-1}\pi x), cos(2^{L-1}\pi x)\}$$
    </p>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/mlp.jpg">
        </div>
    </div>
    <p>
        To actually render a pixel (for loss computation and visualization), I implemented volume rendering. 
        For a given pixel, this essentially integrated over samples along the corresponding ray to accumulate the colors of the points according to their densities. 
        To render a full image, this was done for every pixel. 
        $$\begin{align} \hat{C}(\mathbf{r})=\sum_{i=1}^N T_i\left(1-\exp \left(-\sigma_i \delta_i\right)\right) \mathbf{c}_i, \quad \text {where} \quad T_i=\exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right) \end{align}$$
    </p>
    <p>
        The training loop consisted of fetching points and pixel ground truths from the dataloader, querying the neural network about these points, volume rendering the pixels, and comparing them to the pixel ground truths with MSE loss.
    </p>
    <p>
        Hyperparameters:
    </p>
    <ul>
        <li>Near = 2.0</li>
        <li>Far = 6.0</li>
        <li>Points per ray = 64</li>
        <li>L_position = 10</li>
        <li>L_direction = 4</li>
        <li>Learning rate = 5e-4</li>
        <li>Batch size = 10000</li>
        <li>Total iterations = 1000</li>
    </ul>

    <h4>100 Rays Sampled from Training Set (Viser)</h4>
    <div class="img-group" style="max-width: 1000px">
        <div class="img-item">
            <img src="assets/lego_nerf/viser.jpg">
        </div>
    </div>

    <h4>Intermediate Renders (Validation)</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/lego_nerf/iteration_0.jpg">
            <p><em>Iteration 0</em></p>
        </div>
        <div class="img-item">
            <img src="assets/lego_nerf/iteration_200.jpg">
            <p><em>Iteration 200</em></p>
        </div>
        <div class="img-item">
            <img src="assets/lego_nerf/iteration_400.jpg">
            <p><em>Iteration 400</em></p>
        </div>
        <div class="img-item">
            <img src="assets/lego_nerf/iteration_600.jpg">
            <p><em>Iteration 600</em></p>
        </div>
        <div class="img-item">
            <img src="assets/lego_nerf/iteration_800.jpg">
            <p><em>Iteration 800</em></p>
        </div>
        <div class="img-item">
            <img src="assets/lego_nerf/iteration_1000.jpg">
            <p><em>Iteration 1000</em></p>
        </div>
    </div>

    <h4>Training Curves</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/lego_nerf/training_curves.jpg">
        </div>
    </div>

    <h4>Rendered GIF of Novel Views</h4>
    <div class="img-group" style="max-width: 1000px">
        <div class="img-item">
            <img src="assets/lego_nerf/views.gif">
        </div>
    </div>

    <h3>Part 2.6: Training with Your Own Data</h3>

    <p>
        To generate a NeRF with my own dataset, I had to make minor changes to the hyperparameters.
        I found that near and far parameters of 0.4 and 1.3 worked better, and that I needed to train 10 times longer. 
        I determined this by trying different distances and seeing which parts of the scene were included or excluded too much.
        However, the biggest source of improvement came from retaking photos for my dataset (ensuring that the ArUco tag was centered in each image). 
        Additionally, cropping out borders that resulted from undistorting the images helped a lot too.
    </p>
    <p>
        Hyperparameters:
    </p>
    <ul>
        <li>Near = 0.4</li>
        <li>Far = 1.3</li>
        <li>Points per ray = 64</li>
        <li>L_position = 10</li>
        <li>L_direction = 4</li>
        <li>Learning rate = 5e-4</li>
        <li>Batch size = 10000</li>
        <li>Total iterations = 10000</li>
    </ul>
    <p>
        The spherical GIF of novel views was generated by picking an arbitrary extrinsics matrix from the training set and rotating it around the vertical axis. 
    </p>

    <h4>Intermediate Renders (Validation)</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/octopus_nerf/iteration_0.jpg">
            <p><em>Iteration 0</em></p>
        </div>
        <div class="img-item">
            <img src="assets/octopus_nerf/iteration_2000.jpg">
            <p><em>Iteration 2000</em></p>
        </div>
        <div class="img-item">
            <img src="assets/octopus_nerf/iteration_4000.jpg">
            <p><em>Iteration 4000</em></p>
        </div>
        <div class="img-item">
            <img src="assets/octopus_nerf/iteration_6000.jpg">
            <p><em>Iteration 6000</em></p>
        </div>
        <div class="img-item">
            <img src="assets/octopus_nerf/iteration_8000.jpg">
            <p><em>Iteration 8000</em></p>
        </div>
        <div class="img-item">
            <img src="assets/octopus_nerf/iteration_10000.jpg">
            <p><em>Iteration 10000</em></p>
        </div>
    </div>

    <h4>Training Curves</h4>
    <div class="img-group">
        <div class="img-item">
            <img src="assets/octopus_nerf/training_curves.jpg">
        </div>
    </div>

    <h4>Rendered GIF of Novel Views</h4>
    <div class="img-group" style="max-width: 1000px">
        <div class="img-item">
            <img src="assets/octopus_nerf/views.gif">
        </div>
    </div>
</body>
</html>
